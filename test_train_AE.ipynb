{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"logger\")    #logger名loggerを取得\n",
    "logger.setLevel(logging.DEBUG)  #loggerとしてはDEBUGで\n",
    "\n",
    "#handler1を作成\n",
    "handler1 = logging.StreamHandler()\n",
    "handler1.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)8s %(message)s\"))\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pyenv/versions/3.6.5/lib/python3.6/site-packages/labnote/core.py:71: UserWarning: Please set your script file name to 'script_name'.\n",
      "  warn(\"Please set your script file name to 'script_name'.\")\n",
      "/opt/pyenv/versions/3.6.5/lib/python3.6/site-packages/labnote/core.py:72: UserWarning: ex) note.script_name = xxx.ipynb\n",
      "  warn(\"ex) note.script_name = xxx.ipynb\")\n",
      "/opt/pyenv/versions/3.6.5/lib/python3.6/site-packages/labnote/core.py:73: UserWarning: NOTE: This warning may happen if you are using jupyter in a docker, and access via port-forwarding. Using jupyter with password authentification can be another possibility.\n",
      "  warn(\"NOTE: This warning may happen if you are using jupyter in a docker, and access via port-forwarding. Using jupyter with password authentification can be another possibility.\")\n"
     ]
    }
   ],
   "source": [
    "import labnote as lb\n",
    "if lb.utils.is_executed_on_ipython():\n",
    "    %reload_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "note = lb.Note(arguments=['conf_train_args.yaml',{'config':'conf_AE.yaml'}])\n",
    "\n",
    "\n",
    "# Fix seed\n",
    "if 'seed' in note.params.keys():\n",
    "    np.random.seed(note.params.seed)\n",
    "    _ = torch.manual_seed(note.params.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mnist digits dataset\n",
    "train_data = MNIST(\n",
    "    root='./data/mnist/',\n",
    "    train=True,                                     # this is training data\n",
    "    transform=transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=True,                        # download it if you don't have it\n",
    ")\n",
    "# plot one example\n",
    "print(train_data.train_data.size())     # (60000, 28, 28)\n",
    "print(train_data.train_labels.size())   # (60000)\n",
    "plt.imshow(train_data.train_data[2].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.train_labels[2])\n",
    "plt.show()\n",
    "\n",
    "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=note.params.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4/5 [44928/60000 (100%)]\tLoss: 0.029411\r"
     ]
    }
   ],
   "source": [
    "from sys import stdout\n",
    "import src.utils as my\n",
    "import src.network as net\n",
    "\n",
    "\n",
    "AE = net.AutoEncoder(note.params,logger,input_dim=1, gpu=note.params.gpu)\n",
    "optimizer = eval(note.params.optimizer.format('AE',note.params.learning_rate))\n",
    "\n",
    "epoch = 0\n",
    "global_step = 0\n",
    "AE.train()\n",
    "\n",
    "try:\n",
    "    writer.close()\n",
    "    my.reset_tensorboard(note.params.output)\n",
    "except:\n",
    "    pass\n",
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(note.params.output)\n",
    "\n",
    "def to_gpu(x):\n",
    "    gpu = 0\n",
    "    if note.params.gpu<-1:\n",
    "        return x\n",
    "    if note.params.gpu==-1:\n",
    "        gpu = 0\n",
    "    else:\n",
    "        gpu = note.params.gpu\n",
    "    return x.cuda(gpu)\n",
    "def to_cpu(x):\n",
    "    if note.params.gpu<-1:\n",
    "        return x\n",
    "    return x.cpu()\n",
    "\n",
    "for epoch in range(0,note.params.num_epochs):\n",
    "    for batch_idx, (x,y) in enumerate(train_loader):\n",
    "        x_true = to_gpu(x)\n",
    "        x = to_gpu(2*x-1)\n",
    "        y = to_gpu(y)\n",
    "        optimizer.zero_grad()\n",
    "        z,x_pred = AE.forward(x)\n",
    "        # z = z.detach() # to use z for further calculation.        \n",
    "        \n",
    "        # calc reconstruction loss\n",
    "        l = AE.calc_loss('reconstruction',global_step,x_pred,x_true, writer=writer)\n",
    "        l.backward()\n",
    "        \n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            x_show = np.hstack([to_cpu(x_true[0]).detach(),to_cpu(x_pred[0]).detach()])\n",
    "            writer.add_image('x_pred_epoch%03d'%epoch, x_show, global_step)\n",
    "\n",
    "       \n",
    "        loss_data = to_cpu(l.detach()).numpy()\n",
    "        stdout.write(\n",
    "            'Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\r'.\n",
    "            format(epoch, note.params.num_epochs, batch_idx * len(x), len(train_loader.dataset),\n",
    "                   100. * batch_idx / len(train_loader), loss_data))        \n",
    "        global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save/load test.\n",
    "\n",
    "model_path = os.path.join(note.params.output,'model')\n",
    "AE.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE2 = net.AutoEncoder.load(model_path,logger=logger,input_dim=1, gpu=note.params.gpu)\n",
    "optimizer2 = eval(note.params.optimizer.format('AE2',note.params.learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/pyenv/versions/3.6.5/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4/5 [44928/60000 (100%)]\tLoss: 0.014511\r"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    x_true = to_gpu(x)\n",
    "    x = to_gpu(2*x-1)\n",
    "    optimizer2.zero_grad()\n",
    "    z,x_pred = AE2.forward(x)\n",
    "    # z = z.detach() # to use z for further calculation.        \n",
    "\n",
    "    # calc reconstruction loss\n",
    "    l = AE.calc_loss('reconstruction',global_step,x_pred,x_true)\n",
    "    l.backward()\n",
    "    optimizer2.step()\n",
    "    \n",
    "    loss_data = to_cpu(l.detach()).numpy()\n",
    "    stdout.write(\n",
    "        'Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\r'.\n",
    "        format(epoch, note.params.num_epochs, batch_idx * len(x), len(train_loader.dataset),\n",
    "               100. * batch_idx / len(train_loader), loss_data))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
