{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import gzip\n",
    "import wget\n",
    "import h5py\n",
    "import pickle\n",
    "import urllib\n",
    "import os\n",
    "import skimage\n",
    "import skimage.transform\n",
    "from skimage.io import imread\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import warnings\n",
    "\n",
    "def LoadDataset(name, root, batch_size, split,shuffle=True, style=None, attr=None):\n",
    "    if name == 'mnist':\n",
    "        if split == 'train':\n",
    "            return LoadMNIST(root+'mnist/', batch_size=batch_size, split='train', shuffle=shuffle, scale_32=True)\n",
    "        elif split=='test':\n",
    "            return LoadMNIST(root+'mnist/', batch_size=batch_size, split='test', shuffle=False, scale_32=True)\n",
    "    elif name == 'usps':\n",
    "        if split == 'train':\n",
    "            return LoadUSPS(root+'usps/', batch_size=batch_size, split='train', shuffle=shuffle, scale_32=True)\n",
    "        elif split=='test':\n",
    "            return LoadUSPS(root+'usps/', batch_size=batch_size, split='test', shuffle=False, scale_32=True)\n",
    "    elif name == 'svhn':\n",
    "        if split == 'train':\n",
    "            return LoadSVHN(root+'svhn/', batch_size=batch_size, split='extra', shuffle=shuffle)\n",
    "        elif split=='test':\n",
    "            return LoadSVHN(root+'svhn/', batch_size=batch_size, split='test', shuffle=False)\n",
    "    elif name == 'face':\n",
    "        assert style != None\n",
    "        if split == 'train':\n",
    "            return LoadFace(root, style=style, split='train', batch_size=batch_size,  shuffle=shuffle)\n",
    "        elif split=='test':\n",
    "            return LoadFace(root, style=style, split='test', batch_size=batch_size,  shuffle=False) \n",
    "    elif name=='artificial':\n",
    "        if split == 'train':\n",
    "            return LoadArtificial(root+name+'/', batch_size=batch_size,split='train',shuffle=shuffle)\n",
    "        else:\n",
    "            return LoadArtificial(root+name+'/', batch_size=batch_size,split='test',shuffle=False)\n",
    "            \n",
    "    else:\n",
    "        warnings.warn('unknown dataset name %s'%name)\n",
    "\n",
    "\n",
    "def LoadSVHN(data_root, batch_size=32, split='train', shuffle=True):\n",
    "    if not os.path.exists(data_root):\n",
    "        os.makedirs(data_root)\n",
    "        \n",
    "    filename_train = os.path.join(data_root,'extra_32x32.mat')\n",
    "    filename_test = os.path.join(data_root,'test_32x32.mat')\n",
    "    download = not (os.path.exists(filename_train) and os.path.exists(filename_test))\n",
    "    svhn_dataset = datasets.SVHN(data_root, split=split, download=download,\n",
    "                                   transform=transforms.ToTensor())\n",
    "    return DataLoader(svhn_dataset,batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "\n",
    "def LoadUSPS(data_root, batch_size=32, split='train', shuffle=True, scale_32 = False):\n",
    "    if not os.path.exists(data_root):\n",
    "        os.makedirs(data_root)\n",
    "\n",
    "    usps_dataset = USPS(root=data_root,train=(split=='train'),download=True,scale_32=scale_32)\n",
    "    return DataLoader(usps_dataset,batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "\n",
    "def LoadMNIST(data_root, batch_size=32, split='train', shuffle=True, scale_32 = False):\n",
    "    if not os.path.exists(data_root):\n",
    "        os.makedirs(data_root)\n",
    "\n",
    "    if scale_32:\n",
    "        trans = transforms.Compose([transforms.Resize(size=[32, 32]),transforms.ToTensor()])\n",
    "    else:\n",
    "        trans = transforms.ToTensor()\n",
    "\n",
    "    mnist_dataset = datasets.MNIST(data_root, train=(split=='train'), download=True,\n",
    "                                   transform=trans)\n",
    "    return DataLoader(mnist_dataset,batch_size=batch_size,shuffle=shuffle, drop_last=True)\n",
    "\n",
    "\n",
    "def LoadArtificial(data_root, batch_size=32, split='train',shuffle=True, random_seed=0):\n",
    "    if not os.path.exists(data_root):\n",
    "        os.makedirs(data_root)\n",
    "    \n",
    "    train = (split == 'train')\n",
    "    artificial_dataset = Artificial(\n",
    "        data_root, dataset_size=[5000,1000], dim=16, n_clusters=[1,2,4,8], train=train, \n",
    "                 seed=0, name='artificial')\n",
    "    return DataLoader(artificial_dataset,batch_size=batch_size,shuffle=shuffle,drop_last=True)\n",
    "    \n",
    "\n",
    "def LoadFace(data_root, batch_size=32, split='train', style='photo', attr = None,\n",
    "               shuffle=True, load_first_n = None):\n",
    "\n",
    "    data_root = data_root+'face.h5'\n",
    "    key = '/'.join(['CelebA',split,style])\n",
    "    celeba_dataset = Face(data_root,key,load_first_n)\n",
    "    return DataLoader(celeba_dataset,batch_size=batch_size,shuffle=shuffle,drop_last=True)\n",
    "\n",
    "\n",
    "### USPS Reference : https://github.com/corenel/torchzoo/blob/master/torchzoo/datasets/usps.py\n",
    "class USPS(Dataset):\n",
    "    \"\"\"USPS Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where dataset file exist.\n",
    "        train (bool, optional): If True, resample from dataset randomly.\n",
    "        download (bool, optional): If true, downloads the dataset\n",
    "            from the internet and puts it in root directory.\n",
    "            If dataset is already downloaded, it is not downloaded again.\n",
    "        transform (callable, optional): A function/transform that takes in\n",
    "            an PIL image and returns a transformed version.\n",
    "            E.g, ``transforms.RandomCrop``\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://github.com/mingyuliutw/CoGAN/raw/master/cogan_pytorch/data/uspssample/usps_28x28.pkl\"\n",
    "\n",
    "    def __init__(self, root, train=True, scale_32=False, download=False):\n",
    "        \"\"\"Init USPS dataset.\"\"\"\n",
    "        # init params\n",
    "        self.root = os.path.expanduser(root)\n",
    "\n",
    "        if scale_32:\n",
    "            self.filename = \"usps_32x32.pkl\"\n",
    "        else:\n",
    "            self.filename = \"usps_28x28.pkl\"\n",
    "        self.train = train\n",
    "        # Num of Train = 7438, Num ot Test 1860\n",
    "        self.dataset_size = None\n",
    "\n",
    "        # download dataset.\n",
    "        if download:\n",
    "            self.download()\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\"Dataset not found.\" +\n",
    "                               \" You can use download=True to download it\")\n",
    "\n",
    "        self.train_data, self.train_labels = self.load_samples()\n",
    "        if self.train:\n",
    "            total_num_samples = self.train_labels.shape[0]\n",
    "            indices = np.arange(total_num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            self.train_data = self.train_data[indices[0:self.dataset_size], ::]\n",
    "            self.train_labels = self.train_labels[indices[0:self.dataset_size]]\n",
    "\n",
    "        #self.train_data *= 255.0\n",
    "        #self.train_data = self.train_data.transpose(\n",
    "        #    (0, 2, 3, 1))  # convert to HWC\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get images and target for data loader.\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, label = self.train_data[index, ::], self.train_labels[index]\n",
    "        label = torch.LongTensor([np.int64(label).item()])\n",
    "        # label = torch.FloatTensor([label.item()])\n",
    "        return torch.FloatTensor(img), label[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return size of dataset.\"\"\"\n",
    "        return self.dataset_size\n",
    "\n",
    "    def _check_exists(self):\n",
    "        \"\"\"Check if dataset is download and in right place.\"\"\"\n",
    "        return os.path.exists(os.path.join(self.root, self.filename))\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download dataset.\"\"\"\n",
    "        filename = os.path.join(self.root, 'usps_28x28.pkl')\n",
    "        dirname = os.path.dirname(filename)\n",
    "        if not os.path.isdir(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        if not os.path.isfile(filename):\n",
    "            print(\"Download %s to %s\" % (self.url, os.path.abspath(filename)))\n",
    "            #urllib.request.urlretrieve(self.url, filename)\n",
    "            wget.download(self.url,out=os.path.join(self.root, 'usps_28x28.pkl'))\n",
    "            print(\"[DONE]\")\n",
    "        if not os.path.isfile(os.path.join(self.root, 'usps_32x32.pkl')):\n",
    "            print(\"Resizing USPS 28x28 to 32x32...\")\n",
    "            f = gzip.open(os.path.join(self.root, 'usps_28x28.pkl'), \"rb\")\n",
    "            data_set = pickle.load(f, encoding=\"bytes\")\n",
    "            for d in [0,1]:\n",
    "                tmp = []\n",
    "                for img in range(data_set[d][0].shape[0]):\n",
    "                    tmp.append(np.expand_dims(skimage.transform.resize(data_set[d][0][img].squeeze(),[32,32]),0))\n",
    "                data_set[d][0] = np.array(tmp)\n",
    "            fp=gzip.open(os.path.join(self.root, 'usps_32x32.pkl'),'wb')\n",
    "            pickle.dump(data_set,fp)\n",
    "            print(\"[DONE\")\n",
    "        return\n",
    "\n",
    "    def load_samples(self):\n",
    "        \"\"\"Load sample images from dataset.\"\"\"\n",
    "        filename = os.path.join(self.root, self.filename)\n",
    "        f = gzip.open(filename, \"rb\")\n",
    "        data_set = pickle.load(f, encoding=\"bytes\")\n",
    "        f.close()\n",
    "        if self.train:\n",
    "            images = data_set[0][0]\n",
    "            labels = data_set[0][1]\n",
    "            self.dataset_size = labels.shape[0]\n",
    "        else:\n",
    "            images = data_set[1][0]\n",
    "            labels = data_set[1][1]\n",
    "            self.dataset_size = labels.shape[0]\n",
    "        return images, labels\n",
    "\n",
    "class Face(Dataset):\n",
    "    def __init__(self, root, key, load_first_n = None):\n",
    "\n",
    "        with h5py.File(root,'r') as f:\n",
    "            data = f[key][()]\n",
    "            if load_first_n:\n",
    "                data = data[:load_first_n]\n",
    "        self.imgs = (data/255.0)*2 -1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.imgs[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "import h5py\n",
    "from easydict import EasyDict as edict\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "class Artificial(Dataset):\n",
    "    def __init__(self, root, dataset_size=[5000,1000], dim=16, n_clusters=[1,2,4,8], train=True, \n",
    "                 seed=0, name='artificial'):\n",
    "        self.train = train\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.dataset_size = dataset_size\n",
    "        self.dim = 16\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        \n",
    "        self.filename = os.path.join(root,\"%s.h5\"%name)\n",
    "        self.visualization = os.path.join(root,\"%s.png\"%name)\n",
    "        if not self._check_exists():\n",
    "            X,Y = self._create()\n",
    "            with h5py.File(self.filename, \"w\") as f:\n",
    "                f.create_group('train')\n",
    "                f['train'].create_dataset('X',data=X[:dataset_size[0]])\n",
    "                f['train'].create_dataset('Y',data=Y[:dataset_size[0]])\n",
    "                f.create_group('test')\n",
    "                f['test'].create_dataset('X',data=X[dataset_size[0]:])\n",
    "                f['test'].create_dataset('Y',data=Y[dataset_size[0]:])\n",
    "                f.attrs['dataset_size'] = self.dataset_size\n",
    "                f.attrs['dim'] = self.dim\n",
    "                f.attrs['n_clusters'] = self.n_clusters\n",
    "                f.attrs['image']=self.visualization\n",
    "            \n",
    "        self.train_data,self.train_labels = self.load_samples()\n",
    "\n",
    "    def load_samples(self,train=None):\n",
    "        if train is None:\n",
    "            if self.train:\n",
    "                key = 'train'\n",
    "            else:\n",
    "                key = 'test'\n",
    "        elif train:\n",
    "            key = 'train'\n",
    "        else:\n",
    "            key = 'test'\n",
    "            \n",
    "        with h5py.File(self.filename, 'r') as f:\n",
    "            conf = edict(f.attrs)\n",
    "            data = f[key]['X'].value\n",
    "            labels = f[key]['Y'].value\n",
    "        assert(np.all(self.dataset_size == conf.dataset_size))\n",
    "        assert(self.dim == conf.dim)\n",
    "        assert(np.all(self.n_clusters == conf.n_clusters))\n",
    "        return data,[ls for ls in labels.T]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get images and target for data loader.\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        data = torch.from_numpy(self.train_data[index]) \n",
    "        #label = [torch.from_numpy(l[index]) for l in self.train_labels]\n",
    "        label = [l[index] for l in self.train_labels]\n",
    "        return [data]+label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return size of dataset.\"\"\"\n",
    "        return self.dataset_size[not self.train]\n",
    "\n",
    "    def _check_exists(self):\n",
    "        \"\"\"Check if dataset is download and in right place.\"\"\"\n",
    "        return os.path.exists(self.filename)\n",
    "    \n",
    "    \n",
    "    def _create(self, do_shuffle=True):\n",
    "        N = self.dataset_size[0]+self.dataset_size[1]\n",
    "        \n",
    "        X = []\n",
    "        Y = []\n",
    "        dim_raw = len(self.n_clusters)*2\n",
    "\n",
    "        for c in self.n_clusters:\n",
    "            data = make_blobs(n_samples = N, centers = c, cluster_std=.2, shuffle=False)\n",
    "            X.append(data[0])\n",
    "            Y.append(data[1][:, np.newaxis])\n",
    "            print(c, np.max(data[1]),np.min(data[1]))\n",
    "        X = np.hstack(X)\n",
    "        Y = np.hstack(Y)\n",
    "\n",
    "        print(X.shape)\n",
    "        print(Y.shape)\n",
    "\n",
    "        for i in range(len(self.n_clusters)):\n",
    "            plt.figure(figsize=[dim_raw*2, 3])\n",
    "            plt.subplot(1, 4, i + 1)\n",
    "            plt.scatter(X[:, 2 * i], X[:, 2 * i + 1], 1, Y[:, i])\n",
    "            plt.title('%d-%d-d' % (2 * i, 2 * i + 1))\n",
    "\n",
    "        plt.savefig(self.visualization)\n",
    "\n",
    "        W = np.random.uniform(0,1,(dim_raw,self.dim))\n",
    "        X = np.dot(X,W)\n",
    "\n",
    "        if do_shuffle:\n",
    "            print('shuffle')\n",
    "            X, Y = shuffle(X, Y, random_state=32) \n",
    "            print(Y[:,1])\n",
    "            print(Y[:,2])\n",
    "            print(Y[:,3])\n",
    "\n",
    "        X_max = np.max(X,axis=0).T\n",
    "        X_min = np.min(X,axis=0).T\n",
    "\n",
    "        X -= X_min\n",
    "        X /= (X_max-X_min)\n",
    "        X= 2*(X-0.5) # X has range [1,-1] now.\n",
    "\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
