{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as EDict\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "func_call_pat = re.compile(\"(^|\\s)[a-zA-Z_][a-zA-Z0-9_\\.]*\\(.*\\)\")\n",
    "def is_func_call(string):\n",
    "    return func_call_pat.search(string) is not None\n",
    "\n",
    "format_str_pat = re.compile(\"\\{.*\\}\")\n",
    "def is_format_str(string):\n",
    "    return format_str_pat.search(string) is not None\n",
    "\n",
    "format_model_path = re.compile(\"(.+)_E(\\d+)$\")\n",
    "def parse_model_path(string):\n",
    "    m = format_model_path.match(string)\n",
    "    if m is None:\n",
    "        return string, None\n",
    "    return m.group(1), m.group(2)\n",
    "\n",
    "def parse_params(value, trial = None, conf=None,external_params=None):\n",
    "    if isinstance(value,list):\n",
    "        return type(value)([parse_params(v,trial,conf,external_params) for v in value])\n",
    "    if isinstance(value,dict):\n",
    "        conf = {}\n",
    "        for k,v in value.items():\n",
    "            conf[k] = parse_params(v,trial,conf,external_params)\n",
    "        return conf\n",
    "    res = value\n",
    "    #print(type(res),res,is_func_call(res),is_format_str(res))\n",
    "    try:\n",
    "        if isinstance(res,str):\n",
    "            if len(res)==0:\n",
    "                pass\n",
    "            elif conf is not None and res in conf.keys():\n",
    "                res = conf[res]\n",
    "            elif external_params is not None and res in external_params.keys():\n",
    "                res = external_params[res]\n",
    "            elif not is_format_str(res):\n",
    "                res = eval(res)             \n",
    "    except SyntaxError as e:\n",
    "        res = value\n",
    "        #warnings.warn(\"syntax error in '%s'\"%value)\n",
    "        #raise RuntimeError(e.msg)\n",
    "    except NameError:\n",
    "        res = value\n",
    "    return res\n",
    "\n",
    "def to_list(l):\n",
    "    if l is None:\n",
    "        return []\n",
    "    elif type(l)==list:\n",
    "        return l\n",
    "    return [l]\n",
    "\n",
    "def calc_acc(pred,gt):\n",
    "    gt = gt.cpu() if gt.is_cuda else gt\n",
    "    pred = pred.cpu() if pred.is_cuda else pred    \n",
    "    if gt.shape[-1]==pred.shape[-1]:\n",
    "        acc = float(sum(np.argmax(pred.detach().numpy(),axis=-1)==np.argmax(gt.numpy())))/len(gt)\n",
    "    else:\n",
    "        acc = float(sum(np.argmax(pred.detach().numpy(),axis=-1)==gt.numpy().reshape(-1)))/len(gt)\n",
    "    \n",
    "    return acc\n",
    "def calc_acc_binary(pred,gt):\n",
    "    gt = gt.cpu() if gt.is_cuda else gt\n",
    "    pred = pred.cpu() if pred.is_cuda else pred\n",
    "    coincidence = (pred.detach().numpy()>=0.5)==(gt.numpy()>=0.5)\n",
    "    acc = np.mean(coincidence,axis=(0,1))\n",
    "    return acc\n",
    "\n",
    "class LossWeight:\n",
    "    def __init__(self,init,final=None,start=0,end=0):\n",
    "        self.init = init\n",
    "        self.final = final\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "    def __call__(self,curr_step):\n",
    "        if self.final is None or curr_step <= self.start:\n",
    "            return self.init\n",
    "        if curr_step > self.end:\n",
    "            return self.final\n",
    "        rate = (curr_step - self.start) / (self.end - self.start)\n",
    "        return self.init + rate*(self.final-self.init)\n",
    "    def __str__(self):\n",
    "        if self.final is None:\n",
    "            return str(self.init)\n",
    "        return \"(%d:%f,%d:%f)\"%(self.start,self.init,self.end,self.final)\n",
    "    \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,network_conf,input_dim,logger=None, gpu=0,trial=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim        \n",
    "        self.logger = logger\n",
    "        \n",
    "        self.text_conf = dict(network_conf)\n",
    "        self.conf = parse_params(network_conf, trial, None, network_conf)\n",
    "        if self.logger:\n",
    "            for k,v in self.conf.items():\n",
    "                self.logger.debug(\"%s: %s\"%(k,v))\n",
    "\n",
    "        self.store('sn_params',None)\n",
    "        self.store('LeakyReLU_param',0.2)\n",
    "        \n",
    "        self.models = EDict()\n",
    "        self.losses = EDict()\n",
    "        self.build()\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "        if 'losses' in self.conf.keys():\n",
    "            self.set_losses(self.conf['losses'])\n",
    "        elif self.logger:\n",
    "            self.logger.debug('No loss functions are set to the network.')\n",
    "            \n",
    "        self.gpu = gpu\n",
    "        if gpu < 0:\n",
    "            warnings.warn(\"Run on CPU.\")\n",
    "            return\n",
    "        \n",
    "        self.cuda(gpu)\n",
    "        for k,(_,func) in self.losses.items():\n",
    "            if hasattr(func,'cuda'):\n",
    "                self.losses[k][1] = func.cuda(gpu)                        \n",
    "    \n",
    "    def _save_best_model(self,name,epoch,score):\n",
    "        self.best_score = score\n",
    "        torch.save(self.state_dict(),name+'_best.pth')\n",
    "        if epoch is not None:\n",
    "            with open(name+'_best.txt','w') as f:\n",
    "                f.write(\"%0.10f at epoch %d\\n\"%(score,epoch))\n",
    "    \n",
    "    def save(self,name=None,epoch=None,score=None,keep_prev_epoch=False):\n",
    "        if name is None:\n",
    "            name = type(self).__name__ + \"_model\"\n",
    "        with open(name+'.yaml','w') as f:\n",
    "            yaml.dump(self.text_conf,f)\n",
    "            \n",
    "        if score is not None:\n",
    "            if not hasattr(self,'best_score') or score>self.best_score:\n",
    "                self._save_best_score(name,epoch,score)\n",
    "        \n",
    "        if epoch is not None:\n",
    "            torch.save(self.state_dict(),name+'_E%06d.pth'%epoch)\n",
    "            if epoch>0 and not keep_prev_epoch:\n",
    "                self.remove(name,epoch-1)\n",
    "        else:\n",
    "            torch.save(self.state_dict(),name)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, name, *args, strict=True,**kwargs):\n",
    "        if name is None:\n",
    "            name = cls.__name__ + \"_model\"\n",
    "            \n",
    "        name_chomp, epoch = parse_model_path(name)\n",
    "        print('debug',name,name_chomp,epoch)\n",
    "        \n",
    "        with open(name_chomp+'.yaml','r') as f:\n",
    "            conf = yaml.load(f)\n",
    "            net = cls(conf, *args, **kwargs)\n",
    "            \n",
    "        '''\n",
    "        # check if load is succeed\n",
    "        net_backup = deepcopy(net)\n",
    "        for p,q in zip(net_backup.parameters(),net.parameters()):\n",
    "            axis = tuple(range(len(p.shape)))\n",
    "            p = p.view(-1)\n",
    "            q = q.view(-1)\n",
    "            print(torch.sum(p==q).item()>0)\n",
    "        '''    \n",
    "        net.load_state_dict(torch.load(name+'.pth'),strict=strict)\n",
    "        \n",
    "        '''\n",
    "        print(\"============ LOAD! ============\")\n",
    "        # check if load is succeed\n",
    "        for p,q in zip(net_backup.parameters(),net.parameters()):\n",
    "            axis = tuple(range(len(p.shape)))\n",
    "            p = p.view(-1)\n",
    "            q = q.view(-1)\n",
    "            print(torch.sum(p==q).item()>0)\n",
    "        '''        \n",
    "        return net\n",
    "    \n",
    "    @classmethod\n",
    "    def remove(cls, name,epoch):\n",
    "        if name is None:\n",
    "            name = cls.__name__ + \"_model\"\n",
    "        if epoch is not None:\n",
    "            name += '_E%06d'%epoch\n",
    "        os.remove(name+'.pth')\n",
    "    \n",
    "    def extract_feature(self,data_loader,input_idxs=[0],path=None,ret_val_idx=None,\n",
    "                        ret_gt_idxs=[],**kwargs4forward):\n",
    "        input_idxs = to_list(input_idxs)\n",
    "        ret_gt_idxs = to_list(ret_gt_idxs)\n",
    "        assert(len(input_idxs)>0)\n",
    "        \n",
    "        was_in_training = self.training\n",
    "        self.eval()\n",
    "        \n",
    "        gts = [None] * len(ret_gt_idxs)\n",
    "        \n",
    "        Z = None\n",
    "        for data in data_loader:\n",
    "            for j,idx in enumerate(ret_gt_idxs):\n",
    "                if gts[j] is None:\n",
    "                    gts[j] = data[idx].detach().numpy()\n",
    "                else:\n",
    "                    if len(gts[j].shape)>1:\n",
    "                        gts[j] = np.vstack([gts[j],data[idx].detach().numpy()])\n",
    "                    else:\n",
    "                        gts[j] = np.hstack([gts[j],data[idx].detach().numpy()])\n",
    "                        \n",
    "            inputs = [data[i] for i in input_idxs]\n",
    "            if self.gpu >= 0:\n",
    "                inputs = [x.cuda(self.gpu) for x in inputs]\n",
    "            outputs = self.forward(*inputs,**kwargs4forward)\n",
    "            if ret_val_idx:\n",
    "                if type(outputs) in [list,dict]:\n",
    "                    z = outputs[ret_val_idx].detach()\n",
    "                else:\n",
    "                    z = outputs.detach()\n",
    "                if z.is_cuda:\n",
    "                    z = z.cpu()\n",
    "            else:\n",
    "                assert(not isinstance(outputs,list))\n",
    "            if Z is None:\n",
    "                Z = z\n",
    "            else:\n",
    "                Z = np.vstack([Z,z])\n",
    "\n",
    "        if path:\n",
    "            np.save(path,Z)\n",
    "        \n",
    "        if was_in_training:\n",
    "            # recover the mode.\n",
    "            self.train()\n",
    "        return Z,gts\n",
    "    \n",
    "    def store(self,name,default):\n",
    "        val = default\n",
    "        if name in self.conf.keys():\n",
    "            val = self.conf[name]\n",
    "        setattr(self,name,val)        \n",
    "        \n",
    "    def get_act(self,name):\n",
    "        if name is None or len(name)==0:\n",
    "            return None\n",
    "        if name == 'LeakyReLU':\n",
    "            return nn.LeakyReLU(self.LeakyReLU_param)\n",
    "        if hasattr(nn,name):\n",
    "            # ReLU, Sigmoid, Softmax, Softmax2d, Tanh, Softplus, ...\n",
    "            return eval('nn.%s()'%name)\n",
    "        else:\n",
    "            raise NameError('Unknown activation:'+name)\n",
    "            \n",
    "    def conv_block(self,c_in, c_out, k, s, p, norm='bn', activation=None, dropout=None,transpose=False):\n",
    "        layers = []\n",
    "        if transpose:\n",
    "            layers.append(nn.ConvTranspose2d(c_in, c_out, kernel_size=k, stride=s, padding=p))\n",
    "        else:\n",
    "            layers.append(         nn.Conv2d(c_in, c_out, kernel_size=k, stride=s, padding=p))\n",
    "        if self.sn_params is not None:\n",
    "            layers[-1] = torch.nn.utils.spectral_norm(layers[-1],**self.sn_params)\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout2d(dropout))\n",
    "        if norm == 'bn':\n",
    "            layers.append(nn.BatchNorm2d(c_out))\n",
    "        act = self.get_act(activation)\n",
    "        if act is not None:\n",
    "            layers.append(act)\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    # create a fully connected layer\n",
    "    def fc_block(self,c_in, c_out, norm='bn', activation=None, dropout=None):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(c_in,c_out))\n",
    "        if self.sn_params is not None:\n",
    "            layers[-1] = torch.nn.utils.spectral_norm(layers[-1],**self.sn_params)\n",
    "            \n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        if norm == 'bn':\n",
    "            layers.append(nn.BatchNorm1d(c_out))\n",
    "        act = self.get_act(activation)\n",
    "        if act is not None:\n",
    "            layers.append(act)            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "        \n",
    "    def build_model(self,conf,input_dim,name):\n",
    "        if name in self.models.keys():\n",
    "            raise RuntimeError(\"a model '%s' is already built.\"%name)\n",
    "        model_info = []\n",
    "        block_types = []\n",
    "        params = []\n",
    "        _input_dim = input_dim\n",
    "        \n",
    "        \n",
    "        for i,para in enumerate(conf):  \n",
    "            output_dim = para[1]            \n",
    "            block_name = \"%s_%s_%02d\"%(name,para[0],i)\n",
    "            print(block_name)\n",
    "            if para[0] == 'conv':\n",
    "                block = self.conv_block(_input_dim,output_dim,*para[2:],transpose=False)\n",
    "            elif para[0] in 'trans_conv':\n",
    "                block = self.conv_block(_input_dim,output_dim,*para[2:],transpose=True)\n",
    "            elif para[0] == 'fc':\n",
    "                block = self.fc_block(_input_dim,output_dim,*para[2:])\n",
    "            elif para[0] == 'maxpool':\n",
    "                output_dim = _input_dim\n",
    "                block = nn.MaxPool2d(*para[1:])\n",
    "            elif para[0] == 'avgpool':\n",
    "                output_dim = _input_dim\n",
    "                block = nn.AvgPool2d(*para[1:])\n",
    "            elif para[0] == 'upsample':\n",
    "                output_dim = _input_dim\n",
    "                # ignore size option.\n",
    "                block = nn.Upsample(None, *para[1:])                \n",
    "            else:\n",
    "                if self.logger:\n",
    "                    self.logger.error('Unknown block type: '+para[0])\n",
    "                raise NameError('Unknown block type: '+para[0])\n",
    "            setattr(self,block_name,block)\n",
    "            model_info.append((para[0],block_name,para[1:]))\n",
    "            if self.logger:\n",
    "                self.logger.debug(\"%s: %s -> %s\"%(block_name,_input_dim,output_dim))\n",
    "            _input_dim = output_dim\n",
    "\n",
    "        self.models[name] = {\n",
    "            'input_dim':input_dim,\n",
    "            'output_dim':output_dim,\n",
    "            'blocks': model_info,\n",
    "        }\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward_model(self,name,x):        \n",
    "        for i,(ltype,name,_) in enumerate(self.models[name]['blocks']):\n",
    "            if (ltype == 'fc') and (len(x.size())>2):\n",
    "                batch_size = x.size()[0]        \n",
    "                x = x.view(batch_size,-1)\n",
    "            prev = x.shape\n",
    "            x = getattr(self,name)(x)\n",
    "            #print(name,prev,'->',x.shape)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self,m):\n",
    "        if not hasattr(m,'weights'):\n",
    "            return\n",
    "        inits = []\n",
    "        \n",
    "        classname = m.__class__.__name__\n",
    "        # e.g.) self.conf['initialize'] = {'Conv':'{}.data.normal_(0.0, 0.1)', 'BatchNorm':[0.02,0.01]}\n",
    "        try:\n",
    "            layer_type = next(filter(lambda x: type(m)==x,self.conf['initialize'].keys()))\n",
    "        except StopIteration:       \n",
    "            try:               \n",
    "                layer_type = next(filter(lambda x: classname.find(x)>=0,self.conf['initialize'].keys()))\n",
    "            except StopIteration:\n",
    "                if self.logger:\n",
    "                    logger.info(\"Layer type '%s' is initialized by torch default.\"%classname)\n",
    "                    return\n",
    "        inits = to_list(self.conf['initialize'][layer_type])\n",
    "        \n",
    "        init = inits[0]\n",
    "        if isinstance(init,str):\n",
    "            eval(init.format('m.weight'))\n",
    "        else:\n",
    "            m.weight.data.fill_(init)\n",
    "        \n",
    "        if not hasattr(m,'bias'):\n",
    "            return\n",
    "        \n",
    "        if len(inits)>1 and inits[1] != 'repeat':\n",
    "            init = inits[1]\n",
    "\n",
    "        if isinstance(init,str):\n",
    "            eval(init.format('m.bias'))\n",
    "        else:\n",
    "            m.bias.data.fill_(init)\n",
    "           \n",
    "\n",
    "    def build(self):\n",
    "        raise RuntimeError(\"the function 'build' is a pure-virtual function. This must be implemented in any child class.\")\n",
    "\n",
    "    def set_losses(self,conf):\n",
    "        for k,v in conf.items():\n",
    "            weight = LossWeight(*to_list(v['lambda']))\n",
    "            if not callable(v['func']):\n",
    "                if hasattr(self,v['func']):\n",
    "                    func = getattr(self,v['func'])\n",
    "                else:\n",
    "                    try:\n",
    "                        func = eval(v['func'])\n",
    "                    except NameError:\n",
    "                        continue\n",
    "            else:\n",
    "                func = v['func']\n",
    "            self.losses[k] = [weight,func]\n",
    "    \n",
    "    def calc_loss(self, name, global_step, y_pred, y_true=None, writer=None, acc=False, return4print=False):\n",
    "        weight,func = self.losses[name]\n",
    "        #print(name, type(y_pred),y_pred.dtype,type(y_true),y_true.dtype)\n",
    "        if y_true is not None:\n",
    "            l = func(y_pred,y_true)\n",
    "        else:\n",
    "            l = func(y_pred)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(os.path.join(type(self).__name__,'losses',name),l,global_step)\n",
    "        w = weight(global_step)\n",
    "        outputs = [w*l]\n",
    "        if acc:\n",
    "            if acc == 'categorical':\n",
    "                accuracy = calc_acc(y_pred,y_true)\n",
    "            elif acc == 'binary':\n",
    "                accuracy = calc_acc_binary(y_pred,y_true)\n",
    "            else:\n",
    "                raise RuntimeError('unknown accuracy type.')\n",
    "            outputs.append(accuracy)\n",
    "            writer.add_scalar(os.path.join(type(self).__name__,'accs',name),accuracy,global_step)\n",
    "        if return4print:\n",
    "            if l.is_cuda:\n",
    "                lp = l.cpu().detach()\n",
    "            else:\n",
    "                lp = l.detach()\n",
    "            if weight.final is None:\n",
    "                lp_str = \"%.5f\"%lp\n",
    "            else:\n",
    "                lp_str = \"%.3f x %.5f\"%(w,lp)\n",
    "            if acc:\n",
    "                lp_str += \", acc=%2.1f%%\"%(accuracy*100)\n",
    "            outputs.append(lp_str)\n",
    "        return outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Network):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)        \n",
    "        \n",
    "    def build(self):\n",
    "        depth = -1\n",
    "        if 'AE_depth' in self.conf.keys():\n",
    "            depth = self.conf['AE_depth']\n",
    "        self.build_encoder(self.conf['encoder'])\n",
    "        self.build_decoder(self.conf['decoder'])\n",
    "        \n",
    "    def forward(self,input_x, return_enc = False):\n",
    "        z = self.forward_model('encoder',input_x)\n",
    "        if return_enc:\n",
    "            return z\n",
    "        reconst = self.forward_model('decoder',z)\n",
    "        return z, reconst\n",
    "        \n",
    "    @property\n",
    "    def feature_dim(self):\n",
    "        return self.models['encoder']['output_dim']\n",
    "    \n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self.models['decoder']['output_dim']\n",
    "    \n",
    "    def build_encoder(self,conf,depth=-1):\n",
    "        if depth<=0:\n",
    "            depth = len(conf)\n",
    "        self.build_model(conf[:depth],self.input_dim,'encoder')\n",
    "        \n",
    "    def build_decoder(self,conf,depth=-1):        \n",
    "        if depth<=0:\n",
    "            depth = 0\n",
    "        self.build_model(conf[-depth:],self.feature_dim,'decoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(AutoEncoder):\n",
    "    def __init__(self,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def build(self):\n",
    "        self.encoder = self.build_encoder(self.conf['encoder'])\n",
    "        layer_conf = self.conf['enc_mu']\n",
    "        self.enc_mu = self.build_model(layer_conf)\n",
    "        if 'enc_logvar' in self.conf.keys():\n",
    "            layer_conf = self.conf['enc_logvar']\n",
    "        self.enc_logvar = self.build_model(layer_conf)\n",
    "        \n",
    "        self.decoder = self.build_decoder(self.conf['decoder'])\n",
    "\n",
    "    def forward(self,input_x, return_enc = False):\n",
    "        x = self.forward_model('encoder',input_x)\n",
    "        mu = self.forward_model('enc_mu',x)\n",
    "        logvar = self.forward_model('enc_logvar',x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        if return_enc:\n",
    "            return z\n",
    "        reconst = self.forward_model('decoder',z)\n",
    "        return z, reconst\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def vae_kl(self, mu,logvar):\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    def vae_kl_mean(self, mu,logvar):\n",
    "        return torch.mean(self.vae_kl(mu,logvar),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(Network):\n",
    "    def __init__(self,*args,name=None,**kwargs):\n",
    "        if name is None:\n",
    "            self.name = 'classifier'\n",
    "        else:\n",
    "            self.name = name\n",
    "        super().__init__(*args,**kwargs) \n",
    "        \n",
    "    def build(self):\n",
    "        net_conf = self.conf[self.name]\n",
    "        if self.name+'_depth' in self.conf.keys():\n",
    "            depth = self.conf[self.name+'_depth']\n",
    "            if depth==1:\n",
    "                net_conf = net_conf[-1:]\n",
    "            else:\n",
    "                net_conf = net_conf[:depth-1] + net_conf[-1:]\n",
    "        self.build_model(net_conf,self.input_dim,self.name)\n",
    "        \n",
    "    def forward(self,input_x, return_enc = False):\n",
    "        y = self.forward_model(self.name,input_x)\n",
    "        return y\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
