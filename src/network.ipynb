{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as EDict\n",
    "\n",
    "import re\n",
    "\n",
    "func_call_pat = re.compile(\"(^|\\s)[a-zA-Z_][a-zA-Z0-9_\\.]*\\(.*\\)\")\n",
    "def is_func_call(string):\n",
    "    return func_call_pat.search(string) is not None\n",
    "\n",
    "format_str_pat = re.compile(\"\\{.*\\}\")\n",
    "def is_format_str(string):\n",
    "    return format_str_pat.search(string) is not None\n",
    "\n",
    "\n",
    "def parse_params(value, trial = None, conf=None):\n",
    "    if isinstance(value,list):\n",
    "        return type(value)([parse_params(v,trial,conf) for v in value])\n",
    "    if isinstance(value,dict):\n",
    "        conf = {}\n",
    "        for k,v in value.items():\n",
    "            conf[k] = parse_params(v,trial,conf)\n",
    "        return conf\n",
    "    res = value\n",
    "    #print(type(res),res,is_func_call(res),is_format_str(res))\n",
    "    try:\n",
    "        if isinstance(res,str):\n",
    "            if conf is not None and res in conf.keys():\n",
    "                res = conf[res]\n",
    "            elif not is_format_str(res):\n",
    "                res = eval(res)            \n",
    "    except SyntaxError as e:\n",
    "        warnings.warn(\"syntax error in '%s'\"%value)\n",
    "        raise RuntimeError(e.msg)\n",
    "    except NameError:\n",
    "        res = value\n",
    "    return res\n",
    "\n",
    "def to_list(l):\n",
    "    if l is None:\n",
    "        return []\n",
    "    elif type(l)==list:\n",
    "        return l\n",
    "    return [l]\n",
    "\n",
    "class LossWeight:\n",
    "    def __init__(self,init,final=None,start=0,end=0):\n",
    "        self.init = init\n",
    "        self.final = final\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "    def __call__(self,curr_step):\n",
    "        if self.final is None or curr_step <= self.start:\n",
    "            return self.init\n",
    "        if curr_step > self.end:\n",
    "            return self.final\n",
    "        rate = (curr_step - self.start) / (self.end - self.start)\n",
    "        return self.init + rate*(self.final-self.init)\n",
    "    def __str__(self):\n",
    "        if self.final is None:\n",
    "            return str(self.init)\n",
    "        return \"(%d:%f,%d:%f)\"%(self.start,self.init,self.end,self.final)\n",
    "    \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import yaml\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self,network_conf,logger=None, gpu=0,trial=None):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.logger = logger\n",
    "        \n",
    "        self.text_conf = dict(network_conf)\n",
    "        self.conf = parse_params(network_conf, trial)\n",
    "        if self.logger is not None:\n",
    "            for k,v in self.conf.items():\n",
    "                self.logger.debug(\"%s: %s\"%(k,v))\n",
    "\n",
    "        self.store('sn_params',None)\n",
    "        self.store('LeakyReLU_param',0.2)\n",
    "        \n",
    "        self.models = EDict()\n",
    "        self.losses = EDict()\n",
    "        self.build()\n",
    "\n",
    "        self.init_weights()\n",
    "            \n",
    "        self.set_losses(self.conf['losses'])\n",
    "        \n",
    "        self.gpu = gpu\n",
    "        if gpu < 0:\n",
    "            warnings.warn(\"Run on CPU.\")\n",
    "            return\n",
    "        \n",
    "        for k,(_,func) in self.losses.items():\n",
    "            self.losses[k][1] = func.cuda(gpu)            \n",
    "        self.cuda(gpu)\n",
    "\n",
    "    def save(self,name=None,epoch=None):\n",
    "        if name is None:\n",
    "            name = type(self).__name__ + \"_model\"\n",
    "        with open(name+'.yaml','w') as f:\n",
    "            yaml.dump(self.text_conf,f)\n",
    "            \n",
    "        if epoch is not None:\n",
    "            torch.save(self.state_dict(),name+'%03d.pth'%epoch)\n",
    "            if epoch>0:\n",
    "                self.remove(name,epoch-1)\n",
    "    @classmethod\n",
    "    def load(cls, name, **kwargs):\n",
    "        if name is None:\n",
    "            name = cls.__name__ + \"_model\"\n",
    "        with open(name+'.yaml','r') as f:\n",
    "            conf = yaml.load(f)\n",
    "            net = cls(conf, **kwargs)\n",
    "        net.load_state_dict(torch.load(name+'.pth'))\n",
    "        return net\n",
    "    \n",
    "    @classmethod\n",
    "    def remove(cls, name,epoch):\n",
    "        if name is None:\n",
    "            name = cls.__name__ + \"_model\"\n",
    "        if epoch is not None:\n",
    "            name += '%03d'%epoch\n",
    "        os.remove(name+'.pth')\n",
    "        \n",
    "    def store(self,name,default):\n",
    "        val = default\n",
    "        if name in self.conf.keys():\n",
    "            val = self.conf[name]\n",
    "        setattr(self,name,val)        \n",
    "        \n",
    "    def get_act(self,name):\n",
    "        if name == 'LeakyReLU':\n",
    "            return nn.LeakyReLU(self.LeakyReLU_param)\n",
    "        elif hasattr(nn,name):\n",
    "            # ReLU, Sigmoid, Softmax, Softmax2d, Tanh, Softplus, ...\n",
    "            return eval('nn.%s()'%name)\n",
    "        else:\n",
    "            raise NameError('Unknown activation:'+name)\n",
    "            \n",
    "    def conv_block(self,c_in, c_out, k, s, p, norm='bn', activation=None, dropout=None,transpose=False):\n",
    "        layers = []\n",
    "        if transpose:\n",
    "            layers.append(nn.ConvTranspose2d(c_in, c_out, kernel_size=k, stride=s, padding=p))\n",
    "        else:\n",
    "            layers.append(         nn.Conv2d(c_in, c_out, kernel_size=k, stride=s, padding=p))\n",
    "        if self.sn_params is not None:\n",
    "            layers[-1] = torch.nn.utils.spectral_norm(layers[-1],**self.sn_params)\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout2d(dropout))\n",
    "        if norm == 'bn':\n",
    "            layers.append(nn.BatchNorm2d(c_out))\n",
    "        if activation is not None:\n",
    "            layers.append(self.get_act(activation))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    # create a fully connected layer\n",
    "    def fc_block(self,c_in, c_out, norm='bn', activation=None, dropout=None):\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(c_in,c_out))\n",
    "        if self.sn_params is not None:\n",
    "            layers[-1] = torch.nn.utils.spectral_norm(layers[-1],**self.sn_params)\n",
    "            \n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        if norm == 'bn':\n",
    "            layers.append(nn.BatchNorm1d(c_out))\n",
    "        if activation is not None:\n",
    "            layers.append(self.get_act(activation))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "        \n",
    "    def build_model(self,conf,input_dim,name):\n",
    "        if name in self.models.keys():\n",
    "            raise RuntimeError(\"a model '%s' is already built.\"%name)\n",
    "        model_info = []\n",
    "        block_types = []\n",
    "        params = []\n",
    "        _input_dim = input_dim\n",
    "        \n",
    "        for i,para in enumerate(conf):  \n",
    "            output_dim = para[1]            \n",
    "            block_name = \"%s_%s_%02d\"%(name,para[0],i)\n",
    "            if para[0] == 'conv':\n",
    "                block = self.conv_block(_input_dim,output_dim,*para[2:],transpose=False)\n",
    "            elif para[0] in 'trans_conv':\n",
    "                block = self.conv_block(_input_dim,output_dim,*para[2:],transpose=True)\n",
    "            elif para[0] == 'fc':\n",
    "                block = self.fc_block(_input_dim,output_dim,*para[2:])\n",
    "            elif para[0] == 'maxpool':\n",
    "                output_dim = _input_dim\n",
    "                block = nn.MaxPool2d(*para[1:])\n",
    "            elif para[0] == 'avgpool':\n",
    "                output_dim = _input_dim\n",
    "                block = nn.AvgPool2d(*para[1:])\n",
    "            elif para[0] == 'upsample':\n",
    "                output_dim = _input_dim\n",
    "                # ignore size option.\n",
    "                block = nn.Upsample(None, *para[1:])                \n",
    "            else:\n",
    "                if self.logger is not None:\n",
    "                    self.logger.error('Unknown block type: '+para[0])\n",
    "                raise NameError('Unknown block type: '+para[0])\n",
    "            setattr(self,block_name,block)\n",
    "            model_info.append((para[0],block_name,para[2:]))\n",
    "            self.logger.debug(\"%s: %s -> %s\"%(block_name,_input_dim,output_dim))\n",
    "            _input_dim = output_dim\n",
    "\n",
    "        self.models[name] = {\n",
    "            'input_dim':input_dim,\n",
    "            'output_dim':output_dim,\n",
    "            'blocks': model_info,\n",
    "        }\n",
    "        \n",
    "\n",
    "    \n",
    "    def forward_model(self,name,x):        \n",
    "        for i,(ltype,name,_) in enumerate(self.models[name]['blocks']):\n",
    "            if (ltype == 'fc') and (len(x.size())>2):\n",
    "                batch_size = x.size()[0]        \n",
    "                x = x.view(batch_size,-1)\n",
    "            prev = x.shape\n",
    "            x = getattr(self,name)(x)\n",
    "            #print(name,prev,'->',x.shape)\n",
    "        return x\n",
    "    \n",
    "    def set_losses(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self,m):\n",
    "        if not hasattr(m,'weights'):\n",
    "            return\n",
    "        inits = []\n",
    "        \n",
    "        classname = m.__class__.__name__\n",
    "        # e.g.) self.conf['initialize'] = {'Conv':'{}.data.normal_(0.0, 0.1)', 'BatchNorm':[0.02,0.01]}\n",
    "        try:\n",
    "            layer_type = next(filter(lambda x: type(m)==x,self.conf['initialize'].keys()))\n",
    "        except StopIteration:       \n",
    "            try:               \n",
    "                layer_type = next(filter(lambda x: classname.find(x)>=0,self.conf['initialize'].keys()))\n",
    "            except StopIteration:\n",
    "                logger.info(\"Layer type '%s' is initialized by torch default.\"%classname)\n",
    "                return\n",
    "        inits = to_list(self.conf['initialize'][layer_type])\n",
    "        \n",
    "        init = inits[0]\n",
    "        if isinstance(init,str):\n",
    "            eval(init.format('m.weight'))\n",
    "        else:\n",
    "            m.weight.data.fill_(init)\n",
    "        \n",
    "        if not hasattr(m,'bias'):\n",
    "            return\n",
    "        \n",
    "        if len(inits)>1 and inits[1] != 'repeat':\n",
    "            init = inits[1]\n",
    "\n",
    "        if isinstance(init,str):\n",
    "            eval(init.format('m.bias'))\n",
    "        else:\n",
    "            m.bias.data.fill_(init)\n",
    "           \n",
    "\n",
    "    def build(self):\n",
    "        raise RuntimeError(\"the function 'build' is a pure-virtual function. This must be implemented in any child class.\")\n",
    "\n",
    "    def set_losses(self,conf):\n",
    "        for k,v in conf.items():\n",
    "            weight = LossWeight(*to_list(v['lambda']))\n",
    "            if not callable(v['func']):\n",
    "                func = eval(v['func'])\n",
    "            else:\n",
    "                func = v['func']\n",
    "            self.losses[k] = [weight,func]\n",
    "    \n",
    "    def calc_loss(self,name,global_step,y_pred,y_true=None, writer=None):\n",
    "        weight,func = self.losses[name]\n",
    "        l = func(y_true,y_pred)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(os.path.join(type(self).__name__,'losses',name),l,global_step)\n",
    "            \n",
    "        return weight(global_step) * l\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Network):\n",
    "    def __init__(self,*args,input_dim=3,**kwargs):\n",
    "        self.input_dim = input_dim\n",
    "        super(AutoEncoder,self).__init__(*args,**kwargs)        \n",
    "        \n",
    "    def build(self):\n",
    "        depth = -1\n",
    "        if 'AE_depth' in self.conf.keys():\n",
    "            depth = self.conf['AE_depth']\n",
    "        self.build_encoder(self.conf['encoder'])\n",
    "        self.build_decoder(self.conf['decoder'])\n",
    "        \n",
    "    def forward(self,input_x, return_enc = False):\n",
    "        z = self.forward_model('encoder',input_x)\n",
    "        if return_enc:\n",
    "            return z\n",
    "        reconst = self.forward_model('decoder',z)\n",
    "        return z, reconst\n",
    "        \n",
    "    @property\n",
    "    def feature_dim(self):\n",
    "        return self.models['encoder']['output_dim']\n",
    "    \n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self.models['decoder']['output_dim']\n",
    "    \n",
    "    def build_encoder(self,conf,depth=-1):\n",
    "        if depth<=0:\n",
    "            depth = len(conf)\n",
    "        self.build_model(conf[:depth],self.input_dim,'encoder')\n",
    "        \n",
    "    def build_decoder(self,conf,depth=-1):\n",
    "        if depth<=0:\n",
    "            depth = len(conf)\n",
    "        self.build_model(conf[:depth],self.feature_dim,'decoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(AutoEncoder):\n",
    "    def __init__(self,*args, input_dim=3, **kwargs):\n",
    "        self.input_dim = input_dim\n",
    "        super(VAE,self).__init__(*args, **kwargs)\n",
    "        \n",
    "        self.set_vae_loss()\n",
    "        \n",
    "    def build(self):\n",
    "        self.encoder = self.build_encoder(self.conf['encoder'])\n",
    "        layer_conf = self.conf['enc_mu']\n",
    "        self.enc_mu = self.build_model(layer_conf)\n",
    "        if 'enc_logvar' in self.conf.keys():\n",
    "            layer_conf = self.conf['enc_logvar']\n",
    "        self.enc_logvar = self.build_model(layer_conf)\n",
    "        \n",
    "        self.decoder = self.build_decoder(self.conf['decoder'])\n",
    "\n",
    "    def forward(self,input_x, return_enc = False):\n",
    "        x = self.forward_model('encoder',input_x)\n",
    "        mu = self.forward_model('enc_mu',x)\n",
    "        logvar = self.forward_model('enc_logvar',x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        if return_enc:\n",
    "            return z\n",
    "        reconst = self.forward_model('decoder',z)\n",
    "        return z, reconst\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu        \n",
    "    \n",
    "    def set_vae_loss(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
